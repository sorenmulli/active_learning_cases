{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BO.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkNh7n4jFKAJ",
        "colab_type": "code",
        "outputId": "efc29a76-c5b1-4a65-db8e-f507e8f71005",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "source": [
        "!pip install GPyOpt\n",
        "\n",
        "\n",
        "import torch\n",
        "from torchvision import models, transforms, datasets\n",
        "import time \n",
        "#Training code is inspired heavily from: https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html\n",
        "import os, sys\n",
        "import copy\n",
        "import GPyOpt\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing /root/.cache/pip/wheels/33/1d/87/dc02440831ba986b1547dd11a7dcd44e893b0527083066d869/GPyOpt-1.2.5-cp36-none-any.whl\n",
            "Requirement already satisfied: scipy>=0.16 in /usr/local/lib/python3.6/dist-packages (from GPyOpt) (1.4.1)\n",
            "Collecting GPy>=1.8\n",
            "  Using cached https://files.pythonhosted.org/packages/67/95/976598f98adbfa918a480cb2d643f93fb555ca5b6c5614f76b69678114c1/GPy-1.9.9.tar.gz\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from GPyOpt) (1.17.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from GPy>=1.8->GPyOpt) (1.12.0)\n",
            "Collecting paramz>=0.9.0\n",
            "  Using cached https://files.pythonhosted.org/packages/d8/37/4abbeb78d30f20d3402887f46e6e9f3ef32034a9dea65d243654c82c8553/paramz-0.9.5.tar.gz\n",
            "Requirement already satisfied: decorator>=4.0.10 in /usr/local/lib/python3.6/dist-packages (from paramz>=0.9.0->GPy>=1.8->GPyOpt) (4.4.1)\n",
            "Building wheels for collected packages: GPy, paramz\n",
            "  Building wheel for GPy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for GPy: filename=GPy-1.9.9-cp36-cp36m-linux_x86_64.whl size=2633995 sha256=10309cec54ee4ada66b3072f67d0150157a53782566f5ac73e8aeb0a06ccd14a\n",
            "  Stored in directory: /root/.cache/pip/wheels/5d/36/66/2b58860c84c9f2b51615da66bfd6feeddbc4e04d887ff96dfa\n",
            "  Building wheel for paramz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for paramz: filename=paramz-0.9.5-cp36-none-any.whl size=102552 sha256=bb36adcfe6c857fc4097e5baba0ee3ccb4cc82089e3184dd3cdfa2abc721b8fe\n",
            "  Stored in directory: /root/.cache/pip/wheels/c8/4a/0e/6e0dc85541825f991c431619e25b870d4b812c911214690cf8\n",
            "Successfully built GPy paramz\n",
            "Installing collected packages: paramz, GPy, GPyOpt\n",
            "Successfully installed GPy-1.9.9 GPyOpt-1.2.5 paramz-0.9.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3vQ4B7ZFWoJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def setup_dataset():\n",
        "\n",
        "\tdata_transform = transforms.Compose([\n",
        "\t\t\ttransforms.ToTensor(),\n",
        "\t\t\ttransforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "\t\t])\n",
        "\n",
        "\tbatch_size = 64\n",
        "\n",
        "\tprint(\"Initializing Datasets and Dataloaders...\")\n",
        "\n",
        "\n",
        "\tcifar10 = datasets.CIFAR10('local_data/', train = True,  transform = data_transform, download = True)\n",
        "\tcifar10test = datasets.CIFAR10('local_data/', train = False,  transform = data_transform, download = True)\n",
        "\tcifar10, _ = torch.utils.data.random_split(cifar10, [10000,len(cifar10)-10000])\n",
        "\n",
        "\n",
        "\ttrain_size = int(0.8 * len(cifar10))\n",
        "\ttest_size = len(cifar10) - train_size\n",
        "\tcifar10train, cifar10val = torch.utils.data.random_split(cifar10, [train_size, test_size])\n",
        "\timage_datasets = {'train': cifar10train, 'val': cifar10val}\n",
        "\t\n",
        "\tdataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val']}\n",
        "\ttest_loader = torch.utils.data.DataLoader(cifar10test, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "\treturn dataloaders_dict, test_loader\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yw-mMb0SFeS1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(hyperparameters, model, dataloaders, num_epochs=10):\n",
        "\tin_features, out_features = 25088, 10\n",
        "\t\n",
        "\tp = hyperparameters['p']\n",
        "\thidden_units = hyperparameters['hidden_units']\n",
        "\tactivation_func = hyperparameters['activation_func']\n",
        "\n",
        "\t\n",
        "\tmodel.classifier[0] = torch.nn.Linear(in_features, hidden_units)\n",
        "\tmodel.classifier[1] = activation_func\n",
        "\tmodel.classifier[2] = torch.nn.Dropout(p=p)\n",
        "\tmodel.classifier[3] = torch.nn.Linear(hidden_units, hidden_units)\n",
        "\tmodel.classifier[4] = activation_func\n",
        "\tmodel.classifier[5] = torch.nn.Dropout(p=p)\n",
        "\tmodel.classifier[6] = torch.nn.Linear(hidden_units, out_features)\n",
        "\n",
        "\tdevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\tmodel = model.to(device)\t\n",
        "\n",
        "\tcriterion = torch.nn.CrossEntropyLoss()\n",
        "\toptimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "\n",
        "\tfor epoch in range(num_epochs):\n",
        "\t\tprint('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "\t\tprint('-' * 10)\n",
        "\n",
        "\t\t# Each epoch has a training and validation phase\n",
        "\t\tmodel.train()  # Set model to training mode\n",
        "\n",
        "\t\trunning_loss = 0.0\n",
        "\t\t# Iterate over data.\n",
        "\t\tfor inputs, labels in dataloaders['train']:\n",
        "\t\t\tinputs = inputs.to(device)\n",
        "\t\t\tlabels = labels.to(device)\n",
        "\n",
        "\t\t\t# zero the parameter gradients\n",
        "\t\t\toptimizer.zero_grad()\n",
        "\n",
        "\t\t\twith torch.set_grad_enabled(True):\n",
        "\t\t\t\t# Get model outputs and calculate loss\n",
        "\t\t\t\toutputs = model(inputs)\n",
        "\t\t\t\tloss = criterion(outputs, labels)\n",
        "\n",
        "\n",
        "\t\t\t\tloss.backward()\n",
        "\t\t\t\toptimizer.step()\n",
        "\n",
        "\t\t\t\n",
        "\t\t\trunning_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "\t\tepoch_loss = running_loss / len(dataloaders['train'].dataset)\n",
        "\t\t\n",
        "\n",
        "\t\tprint('Training Loss: {:.4f} '.format(epoch_loss))\n",
        "\t\tprint()\n",
        "\t\t\n",
        "\tmodel.eval()\n",
        "\tcorrects = 0\n",
        "\tfor inputs, labels in dataloaders['val']:\n",
        "\t\tinputs = inputs.to(device)\n",
        "\t\tlabels = labels.to(device)\n",
        "\t\twith torch.set_grad_enabled(False):\n",
        "\t\t\t# Get model outputs and calculate loss\n",
        "\t\t\toutputs = model(inputs)\n",
        "\t\t\t_, preds = torch.max(outputs, 1)\n",
        "\t\t\tcorrects += torch.sum(preds == labels.data)\n",
        "\n",
        "\tval_acc = corrects.double() / len(dataloaders['val'].dataset)\n",
        "\n",
        "\treturn model, float(val_acc)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wq_7jTeKGcoZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATALOADERS, test_loader = setup_dataset()\n",
        "\n",
        "ACTIV = {0: torch.nn.Tanh(),\n",
        "         1: torch.nn.ReLU(),\n",
        "         2: torch.nn.ReLU6(),\n",
        "         3: torch.nn.Sigmoid(),\n",
        "        }\n",
        "\n",
        "\n",
        "def objective_function(x):\n",
        "  MODEL = models.vgg16(pretrained=\"JADAK!\")\n",
        "\n",
        "  for param in MODEL.features.parameters(): param.requires_grad = False\n",
        "\n",
        "  x = x[0]\n",
        "  hyperparameters = {\n",
        "      'hidden_units': int(np.ceil(x[0]*4000))+int(1),\n",
        "      'p': x[1],\n",
        "      'activation_func': ACTIV[int(x[2])]\n",
        "  }\n",
        "  print(hyperparameters)\n",
        "  trained_model, val_acc  = train_model(hyperparameters, MODEL, DATALOADERS)\n",
        "  print(val_acc)\n",
        "  return -val_acc\n",
        "\n",
        "\n",
        "domain = [{'hidden_units'   : 'var_1', 'type': 'continuous', 'domain': (0 , 1)  },\n",
        "          {'p'              : 'var_2', 'type': 'continuous',  'domain': (0 , 1)},\n",
        "          {'activation_func': 'var_3', 'type': 'categorical','domain': tuple(np.arange(4))}]\n",
        "\n",
        "opt = GPyOpt.methods.BayesianOptimization(f = objective_function,   # function to optimize\n",
        "                                              domain = domain,         # box-constrains of the problem\n",
        "                                              acquisition_type = 'EI',      # Select acquisition function MPI, EI, LCB\n",
        "                                             )\n",
        "opt.acquisition.exploration_weight=.1\n",
        "\n",
        "opt.run_optimization(max_iter = 7)\n",
        "x_best = opt.X[np.argmin(opt.Y)]\n",
        "\n",
        "\n",
        "print(\"bedste: \", x_best)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpVR6zziRFbG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "latex_results = \"\"\n",
        "\n",
        "for i, result in enumerate(opt.X[:]):\n",
        "  latex_results += f\"{i+1} & {int(np.ceil(result[0]*4000))} & {result[1]:.4} & {list(ACTIV.keys())[int(result[2])]} & {-float(opt.Y[i])} \\\\\\\\ \\hline \\n\"\n",
        "\n",
        "print(latex_results)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQILOi-JjHHq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "DATALOADERS, test_loader = setup_dataset()\n",
        "\n",
        "ACTIV = {0: torch.nn.Tanh(),\n",
        "         1: torch.nn.ReLU(),\n",
        "         2: torch.nn.ReLU6(),\n",
        "         3: torch.nn.Sigmoid(),\n",
        "        }\n",
        "MODEL = models.vgg16(pretrained=\"sooooo!\")\n",
        "for param in MODEL.features.parameters(): param.requires_grad = False\n",
        "hidden = np.array([100,2000,4000]) ; drop = np.array([0.33, 0.66]) \n",
        "for i in range(3):\n",
        "  for j in range(2):\n",
        "    for k in range(2):\n",
        "      if k == 1:\n",
        "        activation_func = np.random.randint(0, high=4, size = 2)\n",
        "      hyperparameters = {\n",
        "          'hidden_units': int(hidden[i]),\n",
        "          'p': drop[j],\n",
        "          'activation_func': ACTIV[activation_func[k]]\n",
        "      }\n",
        "      print(hyperparameters)\n",
        "      trained_model, val_acc  = train_model(hyperparameters, MODEL, DATALOADERS)\n",
        "      print(val_acc)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZ9KwOWxLYzS",
        "colab_type": "code",
        "outputId": "16d0ae48-1431-4f8e-fd0c-25814a9e06f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "DATALOADERS, test_loader = setup_dataset()\n",
        "\n",
        "def hyperparam_tester(n_hidden, probability, activation_function):\n",
        "    model = models.vgg16(pretrained=\"JADAK!\")\n",
        "    for param in model.features.parameters(): param.requires_grad = False\n",
        "    model, val_acc  = train_model({'hidden_units': n_hidden, 'p': probability, 'activation_func': activation_function}, model, DATALOADERS)\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model.eval()\n",
        "      \n",
        "    corrects = 0\n",
        "    for inputs, labels in test_loader:\n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels.to(device)\n",
        "      with torch.set_grad_enabled(False):\n",
        "        # Get model outputs and calculate loss\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    test_acc = corrects.double() / len(test_loader.dataset)\n",
        "    return test_acc\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initializing Datasets and Dataloaders...\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvXbYEgNyvRM",
        "colab_type": "code",
        "outputId": "ba7e4e1c-3eb1-4d23-b53e-3348e53a7a51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "print(hyperparam_tester(2540, 0.16, torch.nn.Tanh()))\n",
        "print(hyperparam_tester(2403, 0.61, torch.nn.Sigmoid()))\n",
        "print(hyperparam_tester(1150, 0.53, torch.nn.Sigmoid()))\n",
        "print(hyperparam_tester(2000, 0.66, torch.nn.Sigmoid()))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/9\n",
            "----------\n",
            "Training Loss: 1.4856 \n",
            "\n",
            "Epoch 1/9\n",
            "----------\n",
            "Training Loss: 1.1729 \n",
            "\n",
            "Epoch 2/9\n",
            "----------\n",
            "Training Loss: 1.1129 \n",
            "\n",
            "Epoch 3/9\n",
            "----------\n",
            "Training Loss: 1.0901 \n",
            "\n",
            "Epoch 4/9\n",
            "----------\n",
            "Training Loss: 1.0358 \n",
            "\n",
            "Epoch 5/9\n",
            "----------\n",
            "Training Loss: 1.0410 \n",
            "\n",
            "Epoch 6/9\n",
            "----------\n",
            "Training Loss: 1.0321 \n",
            "\n",
            "Epoch 7/9\n",
            "----------\n",
            "Training Loss: 1.0150 \n",
            "\n",
            "Epoch 8/9\n",
            "----------\n",
            "Training Loss: 1.0362 \n",
            "\n",
            "Epoch 9/9\n",
            "----------\n",
            "Training Loss: 1.0258 \n",
            "\n",
            "tensor(0.6179, device='cuda:0', dtype=torch.float64)\n",
            "Epoch 0/9\n",
            "----------\n",
            "Training Loss: 1.3831 \n",
            "\n",
            "Epoch 1/9\n",
            "----------\n",
            "Training Loss: 1.1586 \n",
            "\n",
            "Epoch 2/9\n",
            "----------\n",
            "Training Loss: 1.1290 \n",
            "\n",
            "Epoch 3/9\n",
            "----------\n",
            "Training Loss: 1.0885 \n",
            "\n",
            "Epoch 4/9\n",
            "----------\n",
            "Training Loss: 1.0713 \n",
            "\n",
            "Epoch 5/9\n",
            "----------\n",
            "Training Loss: 1.0563 \n",
            "\n",
            "Epoch 6/9\n",
            "----------\n",
            "Training Loss: 1.0446 \n",
            "\n",
            "Epoch 7/9\n",
            "----------\n",
            "Training Loss: 1.0326 \n",
            "\n",
            "Epoch 8/9\n",
            "----------\n",
            "Training Loss: 1.0113 \n",
            "\n",
            "Epoch 9/9\n",
            "----------\n",
            "Training Loss: 1.0056 \n",
            "\n",
            "tensor(0.6312, device='cuda:0', dtype=torch.float64)\n",
            "Epoch 0/9\n",
            "----------\n",
            "Training Loss: 1.3774 \n",
            "\n",
            "Epoch 1/9\n",
            "----------\n",
            "Training Loss: 1.1388 \n",
            "\n",
            "Epoch 2/9\n",
            "----------\n",
            "Training Loss: 1.0919 \n",
            "\n",
            "Epoch 3/9\n",
            "----------\n",
            "Training Loss: 1.0632 \n",
            "\n",
            "Epoch 4/9\n",
            "----------\n",
            "Training Loss: 1.0357 \n",
            "\n",
            "Epoch 5/9\n",
            "----------\n",
            "Training Loss: 1.0017 \n",
            "\n",
            "Epoch 6/9\n",
            "----------\n",
            "Training Loss: 0.9954 \n",
            "\n",
            "Epoch 7/9\n",
            "----------\n",
            "Training Loss: 0.9863 \n",
            "\n",
            "Epoch 8/9\n",
            "----------\n",
            "Training Loss: 0.9730 \n",
            "\n",
            "Epoch 9/9\n",
            "----------\n",
            "Training Loss: 0.9507 \n",
            "\n",
            "tensor(0.6440, device='cuda:0', dtype=torch.float64)\n",
            "Epoch 0/9\n",
            "----------\n",
            "Training Loss: 1.4013 \n",
            "\n",
            "Epoch 1/9\n",
            "----------\n",
            "Training Loss: 1.2132 \n",
            "\n",
            "Epoch 2/9\n",
            "----------\n",
            "Training Loss: 1.1534 \n",
            "\n",
            "Epoch 3/9\n",
            "----------\n",
            "Training Loss: 1.1177 \n",
            "\n",
            "Epoch 4/9\n",
            "----------\n",
            "Training Loss: 1.1065 \n",
            "\n",
            "Epoch 5/9\n",
            "----------\n",
            "Training Loss: 1.0872 \n",
            "\n",
            "Epoch 6/9\n",
            "----------\n",
            "Training Loss: 1.0858 \n",
            "\n",
            "Epoch 7/9\n",
            "----------\n",
            "Training Loss: 1.0630 \n",
            "\n",
            "Epoch 8/9\n",
            "----------\n",
            "Training Loss: 1.0337 \n",
            "\n",
            "Epoch 9/9\n",
            "----------\n",
            "Training Loss: 1.0463 \n",
            "\n",
            "tensor(0.6307, device='cuda:0', dtype=torch.float64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_NpfMN9SeiJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}